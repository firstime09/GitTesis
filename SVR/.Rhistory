int_result <- intersection %>% group_by(nilai) %>% count()
int_result
int_result$nilai
count()
?count()
table(intersection$nilai) # using table
# using dplyr
# int_result <- intersection %>% group_by(id_polygons) %>% count()
int_result <- intersection %>% group_by(nilai) %>% count()
# Get sample random poins from polygon bbox
set.seed(4)
bbox <- st_as_sfc(st_bbox(polygon))
points <- st_sample(x = bbox, size = 100, type = "random")
points <- st_as_sf(data.frame(id_points = as.character(1:100)), points) # add points ID
points$nilai <-runif(100, min = 0, max = 1)
# Plot polygon + points
plot(polygon, graticule = st_crs(4326), key.pos = 1)
plot(points, pch = 19, col = "black", add = TRUE)
intersection <- st_intersection(x = polygon, y = points)
# Plot intersection
plot(polygon, graticule = st_crs(4326), key.pos = 1)
plot(intersection[1], col = "black", pch = 19, add = TRUE)
# View result
table(intersection$id_polygons) # using table
table(intersection$nilai) # using table
# using dplyr
# int_result <- intersection %>% group_by(id_polygons) %>% count()
int_result <- intersection %>% group_by(nilai) %>% count()
as.data.frame(int_result)[,-3]
# using dplyr
int_result <- intersection %>% group_by(id_polygons) %>% count()
as.data.frame(int_result)[,-3]
intersection
# using dplyr
int_result <- intersection %>% group_by(id_polygons) %>% count(intersection$nilai)
intersection
plot(intersection[1], col = "black", pch = 19, add = TRUE)
plot(intersection[1], col = "black", pch = 19, add = TRUE)
plot(intersection[1], col = "black", pch = 19, add = TRUE)
# Plot intersection
plot(polygon, graticule = st_crs(4326), key.pos = 1)
plot(intersection[1], col = "black", pch = 19, add = TRUE)
int_result <- intersection %>% group_by(id_polygons) %>%
dplyr::summarise(Mean=mean(intersection$nilai, na.rm=TRUE))
int_result
# using dplyr
int_result <- intersection %>% group_by(id_polygons) %>% count(intersection$nilai)
# using dplyr
int_result <- intersection %>% group_by(id_polygons) %>% count()
int_result
int_result$geometry
int_result2 <- intersection %>% group_by(id_polygons) %>%
dplyr::summarise(Mean=mean(intersection$nilai, na.rm=TRUE))
int_result2
mean(intersection$nilai)
points
mean(points$nilai)
plot(intersection)
plot(intersection$nilai)
plot(intersection$id_polygons, intersection$nilai)
plot(intersection$nilai)
plot(intersection)
# Plot polygon + points
plot(polygon, graticule = st_crs(4326), key.pos = 1)
# Inject outliers into data.
cars1 <- cars[1:30, ]  # original data
cars_outliers <- data.frame(speed=c(19,19,20,20,20), dist=c(190, 186, 210, 220, 218))  # introduce outliers.
cars2 <- rbind(cars1, cars_outliers)  # data with outliers.
# Inject outliers into data.
cars1 <- cars[1:30, ]  # original data
cars_outliers <- data.frame(speed=c(19,19,20,20,20), dist=c(190, 186, 210, 220, 218))  # introduce outliers.
cars2 <- rbind(cars1, cars_outliers)  # data with outliers.
View(cars_outliers)
boxplot(cars_outliers)
boxplot(cars1)
boxplot(cars2)
# Plot of data with outliers.
par(mfrow=c(1, 2))
plot(cars2$speed, cars2$dist, xlim=c(0, 28), ylim=c(0, 230), main="With Outliers", xlab="speed", ylab="dist", pch="*", col="red", cex=2)
abline(lm(dist ~ speed, data=cars2), col="blue", lwd=3, lty=2)
# Plot of original data without outliers. Note the change in slope (angle) of best fit line.
plot(cars1$speed, cars1$dist, xlim=c(0, 28), ylim=c(0, 230), main="Outliers removed \n A much better fit!", xlab="speed", ylab="dist", pch="*", col="red", cex=2)
abline(lm(dist ~ speed, data=cars1), col="blue", lwd=3, lty=2)
remove_outliers <- function(x, na.rm = TRUE, ...) {
qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
H <- 1.5 * IQR(x, na.rm = na.rm)
y <- x
y[x < (qnt[1] - H)] <- NA
y[x > (qnt[2] + H)] <- NA
y
}
plot(cars2)
View(cars2)
test<- remove_outliers(cars2)
set.seed(1)
x <- rnorm(100)
x <- c(-10, x, 10)
y <- remove_outliers(x)
## png()
par(mfrow = c(1, 2))
boxplot(x)
boxplot(y)
url <- "http://rstatistics.net/wp-content/uploads/2015/09/ozone.csv"
# alternate source:  https://raw.githubusercontent.com/selva86/datasets/master/ozone.csv
inputData <- read.csv(url)  # import data
outlier_values <- boxplot.stats(inputData$pressure_height)$out  # outlier values.
boxplot(inputData$pressure_height, main="Pressure Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
boxplot(cars2$speed)
boxplot(cars1$speed)
boxplot(cars1$dist)
abline(lm(dist ~ speed, data=cars1), col="blue", lwd=3, lty=2)
# Inject outliers into data.
cars1 <- cars[1:30, ]  # original data
cars_outliers <- data.frame(speed=c(19,19,20,20,20), dist=c(190, 186, 210, 220, 218))  # introduce outliers.
cars2 <- rbind(cars1, cars_outliers)  # data with outliers.
# Plot of data with outliers.
par(mfrow=c(1, 2))
plot(cars2$speed, cars2$dist, xlim=c(0, 28), ylim=c(0, 230), main="With Outliers", xlab="speed", ylab="dist", pch="*", col="red", cex=2)
abline(lm(dist ~ speed, data=cars2), col="blue", lwd=3, lty=2)
# Plot of original data without outliers. Note the change in slope (angle) of best fit line.
plot(cars1$speed, cars1$dist, xlim=c(0, 28), ylim=c(0, 230), main="Outliers removed \n A much better fit!", xlab="speed", ylab="dist", pch="*", col="red", cex=2)
abline(lm(dist ~ speed, data=cars1), col="blue", lwd=3, lty=2)
outlier_values <- boxplot.stats(inputData$pressure_height)$out  # outlier values.
boxplot(inputData$pressure_height, main="Pressure Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
url <- "http://rstatistics.net/wp-content/uploads/2015/09/ozone.csv"
ozone <- read.csv(url)
# For categorical variable
boxplot(ozone_reading ~ Month, data=ozone, main="Ozone reading across months")  # clear pattern is noticeable.
boxplot(ozone_reading ~ Day_of_week, data=ozone, main="Ozone reading for days of week")
View(ozone)
boxplot(ozone_reading ~ pressure_height, data=ozone, main="Boxplot for Pressure height (continuos var) vs Ozone")
boxplot(ozone_reading ~ cut(pressure_height, pretty(inputData$pressure_height)), data=ozone, main="Boxplot for Pressure height (categorial) vs Ozone", cex.axis=0.5)
# For continuous variable (convert to categorical if needed.)
boxplot(ozone_reading ~ pressure_height, data=ozone, main="Boxplot for Pressure height (continuos var) vs Ozone")
boxplot(ozone_reading ~ cut(pressure_height, pretty(inputData$pressure_height)), data=ozone, main="Boxplot for Pressure height (categorial) vs Ozone", cex.axis=0.5)
#Multivariate Model Approach=====
mod <- lm(ozone_reading ~ ., data=ozone)
cooksd <- cooks.distance(mod)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
cooksd
4*mean(cooksd)
# add labels
#Outliers Test====
car::outlierTest(mod)
install.packages("car")
library("car", lib.loc="~/R/win-library/3.5")
# add labels
#Outliers Test====
car::outlierTest(mod)
#Outliers package
set.seed(1234)
y=rnorm(100)
outlier(y)
install.packages("outliers")
library("outliers", lib.loc="~/R/win-library/3.5")
outlier(y)
outlier(y,opposite=TRUE)
dim(y) <- c(20,5)  # convert it to a matrix
outlier(y)
outlier(y,opposite=TRUE)
# Treatment
x <- ozone$pressure_height
qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
caps <- quantile(x, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(x, na.rm = T) x[x < (qnt[1] - H)]  (qnt[2] + H)] <- caps[2]
H <- 1.5 * IQR(x, na.rm = T) x[x < (qnt[1] - H)]  (qnt[2] + H)] <- caps[2]
H <- 1.5 * IQR(x, na.rm = T)
x[x < (qnt[1] - H)] (qnt[2] + H)] <- caps[2]
x[x < (qnt[1] - H)] (qnt[2] + H) <- caps[2]
x[x < (qnt[1] - H)]  (qnt[2] + H)] <- caps[2]
library(MODIStsp)
library(dbscan)
library(readxl)
library(dplyr)
setwd("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR")
# file =read_excel("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR/New_Cidanau_580.xlsx")
file =read_excel("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR/Data_1048_Yoga.xlsx")
head(file)
data <- file[, c(5,7,8,9,10,11,12)]
# data <- file[, c(5:11)]
head(data)
kNNdistplot(data, k = 2)
abline(h=.05, col = "red", lty=2)
# abline(h=.1, col = "red", lty=2)
res <- dbscan(data, eps = .02, minPts = 5)
# abline(h=.1, col = "red", lty=2)
res <- dbscan(data, eps = .05, minPts = 5)
# res <- dbscan(data, eps = .1, minPts = 10)
res
pairs(data, col = res$cluster + 1L)
data$cluster<-res$cluster
file$cluster<-res$cluster
clean<-data %>% filter(cluster > 0)
cleanall<-file %>% filter(cluster > 0)
par(mfrow=c(1,2))
plot(clean$Band_4, clean$frci)
plot(data$Band_4, data$frci)
par(mfrow=c(1,2))
plot(cleanall$Band_4, cleanall$frci)
plot(file$Band_4, file$frci)
cleanall
## Feature Selection
svrdata <- cleanall[,c(5,7,8,9,10,11,12)]
head(svrdata,5)
min(svrdata$frci)
max(svrdata$frci)
svrdata
library(Boruta)
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(frci ~ ., data=na.omit(svrdata), doTrace=2)  # perform Boruta search
library(Boruta)
install.packages("Boruta")
library(Boruta)
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(frci ~ ., data=na.omit(svrdata), doTrace=2)  # perform Boruta search
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% ("Confirmed")])  # collect Confirmed and Tentative variables
# boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  # plot variable importance
set.seed(3033)
intrain <- createDataPartition(y = data$FRCI, p= 0.7, list = FALSE)
training <- data[intrain,]
testing <- data[-intrain,]
dim(training)
dim(testing)
set.seed(3033)
intrain <- createDataPartition(y = svrdata$frci, p= 0.7, list = FALSE)
library(e1071)
library(Boruta)
set.seed(3033)
intrain <- createDataPartition(y = svrdata$frci, p= 0.7, list = FALSE)
library(caret)
set.seed(3033)
intrain <- createDataPartition(y = svrdata$frci, p= 0.7, list = FALSE)
training <- svrdata[intrain,]
testing <- svrdata[-intrain,]
dim(training)
dim(testing)
anyNA(svrdata)
## RMSE
rmse <- function(error)
{
sqrt(mean(error^2))
}
# svr model ==============================================
# if(require(e1071)){
model <- svm(frci ~ . , training)
predictedY <- predict(model, testing)
error <- testing$frci - predictedY  #
svrPredictionRMSE <- rmse(error)  #
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))
# )
print(tuneResult) #
# Draw the first tuning graph
plot(tuneResult)
# Draw the second tuning graph
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,0.4,0.1), cost = 2^(2:9))
)
print(tuneResult)
plot(tuneResult)
# plot(data, pch=16)
tunedModel <- tuneResult$best.model
tunedModelY <- predict(tunedModel, testing)
error <- testing$FRCI - tunedModelY
# this value can  be different because the best model is determined by cross-validation over randomly shuffled data
tunedModelRMSE <- rmse(error)  # 2.219642
error <- testing$frci - tunedModelY
# this value can  be different because the best model is determined by cross-validation over randomly shuffled data
tunedModelRMSE <- rmse(error)  # 2.219642
confusionMatrix(testing$frci, tunedModelY)
tunedModelY
testing$frci
confusionMatrix(testing$frci, predict(tunedModel, testing))
testing$frci
length(testing$frci)
length(tunedModelY)
table(tunedModelY, testing$frci)
accuracy2<-table(tunedModelY, testing$frci)
accuracy2
summary(accuracy2)
RMSE(error)
RMSE(tunedModelY, testing$frci)
# 1. 'Actual' and 'Predicted' data
df <- data.frame(testing$frci, tunedModelY)
# 2.1. Average of actual data
avr_y_actual <- mean(df$y_actual)
# 2.2. Total sum of squares
ss_total <- sum((df$y_actual - avr_y_actual)^2)
# 2.3. Regression sum of squares
ss_regression <- sum((df$y_predicted - avr_y_actual)^2)
# 2.4. Residual sum of squares
ss_residuals <- sum((df$y_actual - df$y_predicted)^2)
# 3. R2 Score
r2 <- 1 - ss_residuals / ss_total
View(df)
# 2.1. Average of actual data
avr_y_actual <- mean(df$testing.frci)
# 2.2. Total sum of squares
ss_total <- sum((df$testing.frci - avr_y_actual)^2)
# 2.2. Total sum of squares
ss_total <- sum((df$testing.frci - avr_y_actual)^2)
# 2.3. Regression sum of squares
ss_regression <- sum((df$tunedModelY - avr_y_actual)^2)
# 2.4. Residual sum of squares
ss_residuals <- sum((df$testing.frci - df$tunedModelY)^2)
# 3. R2 Score
r2 <- 1 - ss_residuals / ss_total
ftable(predict(tunedModel), testing$frci)
ftable(predict(tunedModel, testing), testing$frci)
accuracy2<-ftable(predict(tunedModel, testing), testing$frci)
accuracy3<-ftable(predict(tunedModel, testing), testing$frci)
summary(accuracy3)
## DBSCAN II
db2 <- cleanall[,c(5,7,8,9,10,11,12)]
head(Data)
head(data)
data <- file[, c(5,7,8,9,10,11,12)]
head(data)
# data <- file[, c(5:11)]
data<-data[-1]
data
head(data)
kNNdistplot(data, k = 2)
abline(h=.05, col = "red", lty=2)
# abline(h=.1, col = "red", lty=2)
res <- dbscan(data, eps = .05, minPts = 5)
abline(h=.05, col = "red", lty=2)
abline(h=.02, col = "red", lty=2)
# abline(h=.1, col = "red", lty=2)
res <- dbscan(data, eps = .05, minPts = 5)
# abline(h=.1, col = "red", lty=2)
res <- dbscan(data, eps = .02, minPts = 5)
# res <- dbscan(data, eps = .1, minPts = 10)
res
pairs(data, col = res$cluster + 1L)
data$cluster<-res$cluster
file$cluster<-res$cluster
clean<-data %>% filter(cluster > 0)
cleanall<-file %>% filter(cluster > 0)
par(mfrow=c(1,2))
plot(clean$Band_4, clean$frci)
data <- file[, c(5,7,8,9,10,11,12)]
dataall <- file[, c(5,7,8,9,10,11,12)]
# data <- file[, c(5:11)]
data<-dataall[-1]
head(data)
kNNdistplot(data, k = 2)
abline(h=.02, col = "red", lty=2)
# abline(h=.1, col = "red", lty=2)
res <- dbscan(data, eps = .02, minPts = 5)
# res <- dbscan(data, eps = .1, minPts = 10)
res
pairs(data, col = res$cluster + 1L)
data$cluster<-res$cluster
dataall$cluster<-res$cluster
file$cluster<-res$cluster
clean<-dataall %>% filter(cluster > 0)
cleanall<-file %>% filter(cluster > 0)
par(mfrow=c(1,2))
plot(clean$Band_4, clean$frci)
plot(data$Band_4, data$frci)
par(mfrow=c(1,2))
plot(clean$Band_4, clean$frci)
plot(dataall$Band_4, dataall$frci)
par(mfrow=c(1,2))
plot(cleanall$Band_4, cleanall$frci)
plot(file$Band_4, file$frci)
## DBSCAN II
db2 <- cleanall[,c(5,7,8,9,10,11,12)]
## Feature Selection
svrdata <- cleanall[,c(5,7,8,9,10,11,12)]
library(Boruta)
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(frci ~ ., data=na.omit(svrdata), doTrace=2)  # perform Boruta search
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% ("Confirmed")])  # collect Confirmed and Tentative variables
# boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  # plot variable importance
set.seed(3033)
intrain <- createDataPartition(y = svrdata$frci, p= 0.7, list = FALSE)
training <- svrdata[intrain,]
testing <- svrdata[-intrain,]
dim(training)
dim(testing)
anyNA(svrdata)
## RMSE
rmse <- function(error)
{
sqrt(mean(error^2))
}
# svr model ==============================================
# if(require(e1071)){
model <- svm(frci ~ . , training)
predictedY <- predict(model, testing)
error <- testing$frci - predictedY  #
svrPredictionRMSE <- rmse(error)  #
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))
# )
print(tuneResult) #
# Draw the first tuning graph
plot(tuneResult)
# Draw the second tuning graph
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,0.5,0.1), cost = 2^(2:9))
)
print(tuneResult)
plot(tuneResult)
# plot(data, pch=16)
tunedModel <- tuneResult$best.model
tunedModelY <- predict(tunedModel, testing)
error <- testing$frci - tunedModelY
# this value can  be different because the best model is determined by cross-validation over randomly shuffled data
tunedModelRMSE <- rmse(error)  # 2.219642
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))
# )
print(tuneResult) #
# plot(data, pch=16)
tunedModel <- tuneResult$best.model
tunedModelY <- predict(tunedModel, testing)
error <- testing$frci - tunedModelY
# this value can  be different because the best model is determined by cross-validation over randomly shuffled data
tunedModelRMSE <- rmse(error)  # 2.219642
accuracy<-confusionMatrix(testing$frci, predict(tunedModel, testing))
# 1. 'Actual' and 'Predicted' data
df <- data.frame(testing$frci, tunedModelY)
# 2.1. Average of actual data
avr_y_actual <- mean(df$testing.frci)
# 2.2. Total sum of squares
ss_total <- sum((df$testing.frci - avr_y_actual)^2)
# 2.3. Regression sum of squares
ss_regression <- sum((df$tunedModelY - avr_y_actual)^2)
# 2.4. Residual sum of squares
ss_residuals <- sum((df$testing.frci - df$tunedModelY)^2)
# 3. R2 Score
r2 <- 1 - ss_residuals / ss_total
set.seed(3033)
intrain <- createDataPartition(y = svrdata$frci, p= 0.7, list = FALSE)
training <- svrdata[intrain,]
testing <- svrdata[-intrain,]
dim(training)
dim(testing)
anyNA(svrdata)
## RMSE
rmse <- function(error)
{
sqrt(mean(error^2))
}
# svr model ==============================================
# if(require(e1071)){
model <- svm(frci ~ . , training)
predictedY <- predict(model, testing)
error <- testing$frci - predictedY  #
svrPredictionRMSE <- rmse(error)  #
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))
# )
print(tuneResult) #
# plot(data, pch=16)
tunedModel <- tuneResult$best.model
tunedModelY <- predict(tunedModel, testing)
error <- testing$frci - tunedModelY
# this value can  be different because the best model is determined by cross-validation over randomly shuffled data
tunedModelRMSE <- rmse(error)  # 2.219642
# Draw the second tuning graph
# tuneResult <- tune(svm, frci ~ .,  data = training,
#                    ranges = list(epsilon = seq(0,0.5,0.1), cost = 2^(2:9))
# )
tuneResult <- tune(svm, frci ~ .,  data = training, epsilon = 0.5, cost = 16)
print(tuneResult)
plot(tuneResult)
# plot(data, pch=16)
tunedModel <- tuneResult$best.model
tunedModelY <- predict(tunedModel, testing)
error <- testing$frci - tunedModelY
# this value can  be different because the best model is determined by cross-validation over randomly shuffled data
tunedModelRMSE <- rmse(error)  # 2.219642
# Draw the second tuning graph
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,0.5,0.1), cost = 2^(2:9))
)
print(tuneResult)
plot(tuneResult)
# plot(data, pch=16)
tunedModel <- tuneResult$best.model
tunedModelY <- predict(tunedModel, testing)
error <- testing$frci - tunedModelY
# this value can  be different because the best model is determined by cross-validation over randomly shuffled data
tunedModelRMSE <- rmse(error)  # 2.219642
# }
acc<-tunedModel.accuracy(tunedModelY, testing$frci)
# 1. 'Actual' and 'Predicted' data
df <- data.frame(testing$frci, tunedModelY)
# 2.1. Average of actual data
avr_y_actual <- mean(df$testing.frci)
# 2.2. Total sum of squares
ss_total <- sum((df$testing.frci - avr_y_actual)^2)
# 2.3. Regression sum of squares
ss_regression <- sum((df$tunedModelY - avr_y_actual)^2)
# 2.4. Residual sum of squares
ss_residuals <- sum((df$testing.frci - df$tunedModelY)^2)
# 3. R2 Score
r2 <- 1 - ss_residuals / ss_total
tunedModelRMSE*100
100-16.56026
