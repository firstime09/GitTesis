plot(cars1$speed, cars1$dist, xlim=c(0, 28), ylim=c(0, 230), main="Outliers removed \n A much better fit!", xlab="speed", ylab="dist", pch="*", col="red", cex=2)
abline(lm(dist ~ speed, data=cars1), col="blue", lwd=3, lty=2)
outlier_values <- boxplot.stats(inputData$pressure_height)$out  # outlier values.
boxplot(inputData$pressure_height, main="Pressure Height", boxwex=0.1)
mtext(paste("Outliers: ", paste(outlier_values, collapse=", ")), cex=0.6)
url <- "http://rstatistics.net/wp-content/uploads/2015/09/ozone.csv"
ozone <- read.csv(url)
# For categorical variable
boxplot(ozone_reading ~ Month, data=ozone, main="Ozone reading across months")  # clear pattern is noticeable.
boxplot(ozone_reading ~ Day_of_week, data=ozone, main="Ozone reading for days of week")
View(ozone)
boxplot(ozone_reading ~ pressure_height, data=ozone, main="Boxplot for Pressure height (continuos var) vs Ozone")
boxplot(ozone_reading ~ cut(pressure_height, pretty(inputData$pressure_height)), data=ozone, main="Boxplot for Pressure height (categorial) vs Ozone", cex.axis=0.5)
# For continuous variable (convert to categorical if needed.)
boxplot(ozone_reading ~ pressure_height, data=ozone, main="Boxplot for Pressure height (continuos var) vs Ozone")
boxplot(ozone_reading ~ cut(pressure_height, pretty(inputData$pressure_height)), data=ozone, main="Boxplot for Pressure height (categorial) vs Ozone", cex.axis=0.5)
#Multivariate Model Approach=====
mod <- lm(ozone_reading ~ ., data=ozone)
cooksd <- cooks.distance(mod)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels
cooksd
4*mean(cooksd)
# add labels
#Outliers Test====
car::outlierTest(mod)
install.packages("car")
library("car", lib.loc="~/R/win-library/3.5")
# add labels
#Outliers Test====
car::outlierTest(mod)
#Outliers package
set.seed(1234)
y=rnorm(100)
outlier(y)
install.packages("outliers")
library("outliers", lib.loc="~/R/win-library/3.5")
outlier(y)
outlier(y,opposite=TRUE)
dim(y) <- c(20,5)  # convert it to a matrix
outlier(y)
outlier(y,opposite=TRUE)
# Treatment
x <- ozone$pressure_height
qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
caps <- quantile(x, probs=c(.05, .95), na.rm = T)
H <- 1.5 * IQR(x, na.rm = T) x[x < (qnt[1] - H)]  (qnt[2] + H)] <- caps[2]
H <- 1.5 * IQR(x, na.rm = T) x[x < (qnt[1] - H)]  (qnt[2] + H)] <- caps[2]
H <- 1.5 * IQR(x, na.rm = T)
x[x < (qnt[1] - H)] (qnt[2] + H)] <- caps[2]
x[x < (qnt[1] - H)] (qnt[2] + H) <- caps[2]
x[x < (qnt[1] - H)]  (qnt[2] + H)] <- caps[2]
library(MODIStsp)
library(MODIStsp)
library("MODIStsp", lib.loc="~/R/win-library/3.5")
MODIStsp()
library(readxl)
library(caret)
library(kernlab)
library(ROCR)
library(e1071)
library(rgdal)
library(raster)
library(dismo)
library(readxl)
library(caret)
library(kernlab)
library(ROCR)
library(e1071)
library(rgdal)
library(raster)
library(readxl)
library(caret)
library(kernlab)
library(ROCR)
library(e1071)
library(rgdal)
library(raster)
library(dismo)
data_plot <- read_excel("D:/FORESTS2020/TRAINING/R/Data/FRCI/data plot.xlsx")
view(data_plot)
View(data_plot)
View(data_plot)
head(data_plot)
data_all= data_plot[-c(1,2,3,4,6,13)]
head(data_plot)
head(data_all)
## Feature Selection
library(Boruta)
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(FRCI ~ ., data=na.omit(data), doTrace=2)  # perform Boruta search
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% ("Confirmed")])  # collect Confirmed and Tentative variables
str(data)
## Feature Selection
library(Boruta)
head(data_all)
data<-data_all
str(data)
## Feature Selection
library(Boruta)
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(FRCI ~ ., data=na.omit(data), doTrace=2)  # perform Boruta search
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(FRCI ~ ., data=na.omit(data), doTrace=2)  # perform Boruta search
data_plot <- read_excel("D:/FORESTS2020/TRAINING/R/Data/FRCI/data plot.xlsx")
predictors <- stack(list.files(file.path("D:/FORESTS2020/TRAINING/R/Data/FRCI/TRASH"), pattern='img$', full.names=TRUE ))
predictors
# boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% ("Confirmed")])  # collect Confirmed and Tentative variables
# boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  # plot variable importance
set.seed(3033)
intrain <- createDataPartition(y = data$FRCI, p= 0.7, list = FALSE)
training <- data[intrain,]
testing <- data[-intrain,]
dim(training)
dim(testing)
anyNA(data)
anyNA(data)
## RMSE
rmse <- function(error)
{
sqrt(mean(error^2))
}
# names(data)<- c("X", "Y")
plot(data)
head(data)
model <- svm(FRCI ~ ., training)
predictedY <- predict(model, testing)
error <- testing$FRCI - predictedY  #
svrPredictionRMSE <- rmse(error)  #
head(data)
model
tuneResult <- tune(svm, FRCI ~ .,  data = training,
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))
tuneResult
library(readxl)
library(caret)
library(e1071)
library(rgdal)
library(raster)
library(dismo)
data_plot <- read_excel("D:/FORESTS2020/TRAINING/R/SVR/data plot.xlsx")
predictors <- stack(list.files(file.path("D:/FORESTS2020/TRAINING/R/SVR/Raster"), pattern='img$', full.names=TRUE ))
head(data_plot)
data_all= data_plot[-c(1,2,3,4,6,13)]
head(data_all)
data<-data_all
str(data)
## Feature Selection
library(Boruta)
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(FRCI ~ ., data=na.omit(data), doTrace=2)  # perform Boruta search
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% ("Confirmed")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  # plot variable importance
# Divide data to training and testing ===============================
set.seed(3033)
intrain <- createDataPartition(y = data$FRCI, p= 0.7, list = FALSE)
training <- data[intrain,]
testing <- data[-intrain,]
dim(training)
dim(testing)
anyNA(data)
## RMSE
rmse <- function(error)
{
sqrt(mean(error^2))
}
# names(data)<- c("X", "Y")
plot(data)
# svr model ==============================================
model <- svm(FRCI ~ ., training)
# svr model ==============================================
model <- svm(FRCI ~ ., training)
predictedY <- predict(model, testing)
error <- testing$FRCI - predictedY  #
svrPredictionRMSE <- rmse(error)  #
tuneResult <- tune(svm, FRCI ~ .,  data = training,
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))
print(tuneResult)
# Draw the first tuning graph
plot(tuneResult)
# Draw the second tuning graph
tuneResult <- tune(svm, FRCI ~ .,  data = training,
ranges = list(epsilon = seq(0,0.01,0.1), cost = 2^(2:9))
)
print(tuneResult)
plot(tuneResult)
# Draw the first tuning graph
plot(tuneResult)
# Draw the first tuning graph
plot(tuneResult)
# Draw the second tuning graph
tuneResult <- tune(svm, FRCI ~ .,  data = training,
ranges = list(epsilon = seq(0,0.01,0.1), cost = 2^(2:9))
)
print(tuneResult)
tunedModel <- tuneResult$best.model
tunedModelY <- predict(tunedModel, testing)
error <- testing$FRCI - tunedModelY
tunedModelRMSE <- rmse(error)
library(dbscan)
library(readxl)
library(dplyr)
library(e1071)
library(Boruta)
library(caret)
library(raster)
library(dismo)
setwd("D:/FORESTS2020/TRAINING/R/SVR")
file =read_excel("All_Plot_Topo.xlsx")
# file<-file[,-c(3,4,11,12,13)]
file<-file[,-c(3,4)]
head(file)
dataall <- file[,-c(9,10,11)]
data<-file[,-c(9,10,11)]
head(data)
# data<-data[-5]
### next step
# cleanall$kelas <-cleanall$Class
number <-data %>%
group_by(class) %>%
summarize(n())
sample <-data%>%
group_by(class)%>%
sample_n(min(number$`n()`))
head(sample)
sample<-sample[-1]
##2
# sample<-data[-1]
head(sample)
# y <-round(sample[1], 3)
# dataSample<- as.data.frame( scale(sample))
lst <- as.data.frame(lapply(sample, function(x) round((x-min(x))/(max(x)-min(x)), 3)))
head(lst)
# dataSample<- cbind(y,lst)
dataSample<- lst
head(dataSample)
kNNdistplot(dataSample, k = 5)
abline(h=0.4, col = "red", lty=2)
# abline(h=.1, col = "red", lty=2)
# res <- dbscan(data, eps = .05, minPts = 5)
res <- dbscan(dataSample, eps =0.4 , minPts = 5)
res
pairs(dataSample, col = res$cluster + 1L)
dataSample$cluster<-res$cluster
cleanall<-dataSample %>% filter(cluster > 0)
par(mfrow=c(1,2))
plot(cleanall$Band_4, cleanall$frci)
plot(dataSample$Band_4, dataSample$frci)
## Feature Selection
# svrdata <- cleanall[,c(5,7,8,9,10,11,12)]
svrdata <- cleanall
svrdata <- cleanall[-8]
head(svrdata)
library(Boruta)
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(frci ~ ., data=na.omit(svrdata), doTrace=2)  # perform Boruta search
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% ("Confirmed")])  # collect Confirmed and Tentative variables
# boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  # plot variable importance
set.seed(3033)
intrain <- createDataPartition(y = svrdata$frci, p= 0.7, list = FALSE)
training <- svrdata[intrain,]
testing <- svrdata[-intrain,]
dim(training)
dim(testing)
anyNA(svrdata)
## RMSE
rmse <- function(error)
{
sqrt(mean(error^2))
}
# svr model ==============================================
# if(require(e1071)){
model <- svm(frci ~ . , training)
predictedY <- predict(model, testing)
error <- testing$frci - predictedY  #
svrPredictionRMSE <- rmse(error)  #
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))
# )
print(tuneResult) #
# Draw the first tuning graph
plot(tuneResult)
# Draw the second tuning graph
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,0.3,0.1), cost = 2^(2:9))
)
print(tuneResult)
plot(tuneResult)
# plot(data, pch=16)
tunedModel <- tuneResult$best.model
tunedModelY <- predict(tunedModel, testing)
# 1. 'Actual' and 'Predicted' data
df <- data.frame(testing$frci, tunedModelY)
# 2.1. Average of actual data
avr_y_actual <- mean(df$testing.frci)
# 2.2. Total sum of squares
ss_total <- sum((df$testing.frci - avr_y_actual)^2)
# 2.3. Regression sum of squares
ss_regression <- sum((df$tunedModelY - avr_y_actual)^2)
# 2.4. Residual sum of squares
ss_residuals <- sum((df$testing.frci - df$tunedModelY)^2)
# 3. R2 Score
r2 <- 1 - ss_residuals / ss_total
error <- testing$frci - tunedModelY
# this value can  be different because the best model is determined by cross-validation over randomly shuffled data
tunedModelRMSE <- rmse(error)  # 2.219642
library(dismo)
setwd("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR")
# file =read_excel("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR/New_Cidanau_580.xlsx")
file =read_excel("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR/Data_1048_Yoga.xlsx")
head(file)
dataall <- file[, c(5,7,8,9,10,11,12)]
# data <- file[, c(5:11)]
data<-dataall[-1]
data<-dataall
head(data)
kNNdistplot(data, k = 5)
abline(h=.05, col = "red", lty=2)
# abline(h=.1, col = "red", lty=2)
res <- dbscan(data, eps = .05, minPts = 5)
# res <- dbscan(data, eps = .1, minPts = 10)
res
pairs(data, col = res$cluster + 1L)
dataall$cluster<-res$cluster
file$cluster<-res$cluster
clean<-dataall %>% filter(cluster > 0)
cleanall<-file %>% filter(cluster > 0)
par(mfrow=c(1,2))
plot(clean$Band_4, clean$frci)
plot(dataall$Band_4, dataall$frci)
par(mfrow=c(1,2))
plot(cleanall$Band_4, cleanall$frci)
plot(file$Band_4, file$frci)
## Feature Selection
svrdata <- cleanall[,c(5,7,8,9,10,11,12)]
svrdata <- data
library(Boruta)
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(frci ~ ., data=na.omit(svrdata), doTrace=2)  # perform Boruta search
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% ("Confirmed")])  # collect Confirmed and Tentative variables
# boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  # plot variable importance
set.seed(3033)
intrain <- createDataPartition(y = svrdata$frci, p= 0.7, list = FALSE)
training <- svrdata[intrain,]
testing <- svrdata[-intrain,]
dim(training)
dim(testing)
anyNA(svrdata)
## RMSE
rmse <- function(error)
{
sqrt(mean(error^2))
}
# svr model ==============================================
# if(require(e1071)){
model <- svm(frci ~ . , training)
predictedY <- predict(model, testing)
error <- testing$frci - predictedY  #
svrPredictionRMSE <- rmse(error)  #
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))
# )
print(tuneResult) #
# Draw the first tuning graph
plot(tuneResult)
# Draw the second tuning graph
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,0.4,0.1), cost = 2^(2:9))
)
tuneResult_test <- tune(svm, frci ~ .,  data = training, epsilon = 0.1, cost = 1)
print(tuneResult)
plot(tuneResult)
# plot(data, pch=16)
tunedModel <- tuneResult$best.model
tunedModelY <- predict(tunedModel, testing)
# 1. 'Actual' and 'Predicted' data
df <- data.frame(testing$frci, tunedModelY)
# 2.1. Average of actual data
avr_y_actual <- mean(df$testing.frci)
# 2.2. Total sum of squares
ss_total <- sum((df$testing.frci - avr_y_actual)^2)
# 2.3. Regression sum of squares
ss_regression <- sum((df$tunedModelY - avr_y_actual)^2)
# 2.4. Residual sum of squares
ss_residuals <- sum((df$testing.frci - df$tunedModelY)^2)
# 3. R2 Score
r2 <- 1 - ss_residuals / ss_total
tunedModel_test <- tuneResult_test$best.model
tunedModel_test <- tuneResult_test$best.model
error <- testing$frci - tunedModelY
# this value can  be different because the best model is determined by cross-validation over randomly shuffled data
tunedModelRMSE <- rmse(error)  # 2.219642
setwd("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR")
# file =read_excel("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR/New_Cidanau_580.xlsx")
file =read_excel("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR/Data_1048_Yoga.xlsx")
head(file)
dataall <- file[, c(5,7,8,9,10,11,12)]
# data <- file[, c(5:11)]
data<-dataall[-1]
data<-dataall
head(data)
kNNdistplot(data, k = 5)
abline(h=.015, col = "red", lty=2)
head(data)
# setwd("D:/FORESTS2020/TRAINING/R/SVR")
# file =read_excel("All_Plot_Topo.xlsx")
setwd("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR")
file =read_excel("D:/FORESTS2020/GITHUB/Plugin/GitTesis/SVR/Data_1048_Yoga.xlsx")
head(file)
file<-file[,-c(1,2,4,6,13)]
# file<-file[,-c(3,4)]
head(file)
# dataall <- file[,-c(9,10,11)]
dataall <- file
# data<-file[,-c(9,10,11)]
data<-file
head(data)
# data<-data[-5]
### next step
# cleanall$kelas <-cleanall$Class
number <-data %>%
group_by(class) %>%
summarize(n())
sample <-data%>%
group_by(class)%>%
sample_n(min(number$`n()`))
head(sample)
sample<-sample[-1]
head(data)
# data<-data[-5]
### next step
# cleanall$kelas <-cleanall$Class
number <-data %>%
group_by(Class) %>%
summarize(n())
sample <-data%>%
group_by(Class)%>%
sample_n(min(number$`n()`))
head(sample)
sample<-sample[-1]
##2
# sample<-data[-1]
head(sample)
# y <-round(sample[1], 3)
# dataSample<- as.data.frame( scale(sample))
lst <- as.data.frame(lapply(sample, function(x) round((x-min(x))/(max(x)-min(x)), 3)))
head(lst)
# dataSample<- cbind(y,lst)
dataSample<- lst
head(dataSample)
kNNdistplot(dataSample, k = 5)
abline(h=0.2, col = "red", lty=2)
# abline(h=.1, col = "red", lty=2)
# res <- dbscan(data, eps = .05, minPts = 5)
res <- dbscan(dataSample, eps =0.2 , minPts = 5)
res
pairs(dataSample, col = res$cluster + 1L)
dataSample$cluster<-res$cluster
cleanall<-dataSample %>% filter(cluster > 0)
par(mfrow=c(1,2))
plot(cleanall$Band_4, cleanall$frci)
plot(dataSample$Band_4, dataSample$frci)
## Feature Selection
# svrdata <- cleanall[,c(5,7,8,9,10,11,12)]
svrdata <- cleanall
svrdata <- cleanall[-8]
head(svrdata)
library(Boruta)
# Decide if a variable is important or not using Boruta
boruta_output <- Boruta(frci ~ ., data=na.omit(svrdata), doTrace=2)  # perform Boruta search
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% ("Confirmed")])  # collect Confirmed and Tentative variables
# boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  # plot variable importance
set.seed(3033)
intrain <- createDataPartition(y = svrdata$frci, p= 0.7, list = FALSE)
training <- svrdata[intrain,]
testing <- svrdata[-intrain,]
dim(training)
dim(testing)
anyNA(svrdata)
## RMSE
rmse <- function(error)
{
sqrt(mean(error^2))
}
# svr model ==============================================
# if(require(e1071)){
model <- svm(frci ~ . , training)
predictedY <- predict(model, testing)
error <- testing$frci - predictedY  #
svrPredictionRMSE <- rmse(error)  #
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))
# )
print(tuneResult) #
# Draw the first tuning graph
plot(tuneResult)
# Draw the second tuning graph
tuneResult <- tune(svm, frci ~ .,  data = training,
ranges = list(epsilon = seq(0,0.4,0.1), cost = 2^(2:9))
)
